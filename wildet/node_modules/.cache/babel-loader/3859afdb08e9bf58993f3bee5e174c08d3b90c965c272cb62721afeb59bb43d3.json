{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, reshape, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(x, mean, variance, beta, gamma) {\n  let epsilon = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1e-3;\n  let out;\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n  } else {\n    throw new NotImplementedError(`batchNormalization is not implemented for array of rank ${x.rank} ` + `yet`);\n  }\n  return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  let epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(() => {\n    const meanAndVariance = tfc.moments(x, reductionAxes);\n    const mean = meanAndVariance.mean;\n    const variance = meanAndVariance.variance;\n    const normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  let epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(() => {\n    const meanAndVariance = tfc.moments(x, reductionAxes);\n    const mean = meanAndVariance.mean;\n    const variance = meanAndVariance.variance;\n    const targetShape = [];\n    for (const axis of math_utils.range(0, x.rank)) {\n      if (reductionAxes.indexOf(axis) !== -1) {\n        targetShape.push(1);\n      } else {\n        targetShape.push(x.shape[axis]);\n      }\n    }\n    const broadcastMean = reshape(mean, targetShape);\n    const broadcastVariance = reshape(variance, targetShape);\n    const broadcastGamma = gamma == null ? null : reshape(gamma, targetShape);\n    const broadcastBeta = beta == null ? null : reshape(beta, targetShape);\n    const normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  let epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  }\n}\nexport class BatchNormalization extends Layer {\n  constructor(args) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n    this.supportsMasking = true;\n    this.axis = args.axis == null ? -1 : args.axis;\n    this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.movingMeanInitializer = getInitializer(args.movingMeanInitializer || 'zeros');\n    this.movingVarianceInitializer = getInitializer(args.movingVarianceInitializer || 'ones');\n    this.betaConstraint = getConstraint(args.betaConstraint);\n    this.gammaConstraint = getConstraint(args.gammaConstraint);\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n  }\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const axis = this.axis >= 0 ? this.axis : this.axis + inputShape.length;\n    const dim = inputShape[axis];\n    if (dim == null) {\n      throw new ValueError(`Axis ${axis} of input tensor should have a defined dimension but ` + `the layer received an input with shape ` + `${JSON.stringify(inputShape)}.`);\n    }\n    this.inputSpec = [new InputSpec({\n      ndim: inputShape.length,\n      axes: {\n        [axis]: dim\n      }\n    })];\n    const shape = [dim];\n    if (this.scale) {\n      this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n    }\n    if (this.center) {\n      this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n    }\n    this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n    this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n    this.built = true;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const ndim = inputShape.length;\n      const reductionAxes = math_utils.range(0, ndim);\n      const axis = this.axis >= 0 ? this.axis : this.axis + ndim;\n      reductionAxes.splice(axis, 1);\n      const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n      broadcastShape[axis] = inputShape[axis];\n      const sortedReductionAxes = reductionAxes.slice();\n      sortedReductionAxes.sort();\n      const needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n      const normalizeInference = () => {\n        if (needsBroadcasting) {\n          const broadcastMovingMean = reshape(this.movingMean.read(), broadcastShape);\n          const broadcastMovingVariance = reshape(this.movingVariance.read(), broadcastShape);\n          const broadcastBeta = this.center ? reshape(this.beta.read(), broadcastShape) : null;\n          const broadcastGamma = this.scale ? reshape(this.gamma.read(), broadcastShape) : null;\n          return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, this.epsilon);\n        } else {\n          return batchNormalization(input, this.movingMean.read(), this.movingVariance.read(), this.beta == null ? null : this.beta.read(), this.gamma == null ? null : this.gamma.read(), this.epsilon);\n        }\n      };\n      if (!training) {\n        return normalizeInference();\n      }\n      const [normedTraining, mean, variance] = normalizeBatchInTraining(input, this.gamma.read(), this.beta.read(), reductionAxes, this.epsilon);\n      const doMovingAverage = (variable, value, momentum) => {\n        tfc.tidy(() => {\n          const decay = 1 - momentum;\n          const origValue = variable.read();\n          const updateDelta = tfc.mul(tfc.sub(origValue, value), decay);\n          variable.write(tfc.sub(origValue, updateDelta));\n        });\n      };\n      // Perform updates to moving mean and moving variance for training.\n      // Porting Note: In PyKeras, these updates to `movingMean` and\n      //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n      //   `update`s using the `add_update()` method. Here we do it imperatively\n      //   and encapsulate the updates in a function that is invoked\n      //   immediately.\n      const updateMovingMeanAndVariance = () => {\n        doMovingAverage(this.movingMean, mean, this.momentum);\n        doMovingAverage(this.movingVariance, variance, this.momentum);\n      };\n      updateMovingMeanAndVariance();\n      return normedTraining;\n    });\n  }\n  getConfig() {\n    const config = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport class LayerNormalization extends Layer {\n  constructor(args) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n    this.axis = args.axis == null ? -1 : args.axis;\n    if (typeof this.axis === 'number') {\n      if (!Number.isInteger(this.axis)) {\n        throw new Error(`Expected axis to be an integer, but received ${this.axis}`);\n      }\n    } else if (Array.isArray(this.axis)) {\n      for (const axis of this.axis) {\n        if (!Number.isInteger(axis)) {\n          throw new Error(`Expected axis to be an array of integers, ` + `but received ${JSON.stringify(this.axis)}`);\n        }\n      }\n    } else {\n      throw new Error(`Expected axis to be an integer or an array of integers, ` + `but received ${JSON.stringify(this.axis)}`);\n    }\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    this.supportsMasking = true;\n  }\n  build(inputShape) {\n    inputShape = getExactlyOneShape(inputShape);\n    const nDims = inputShape.length;\n    // Convert axis to array and resolve negatives.\n    if (typeof this.axis === 'number') {\n      this.axis = [this.axis];\n    }\n    for (let i = 0; i < this.axis.length; ++i) {\n      if (this.axis[i] < 0) {\n        this.axis[i] += nDims;\n      }\n    }\n    // Further validate axes.\n    for (const axis of this.axis) {\n      if (axis < 0 || axis >= nDims) {\n        throw new Error(`Invalid axis: ${axis}`);\n      }\n    }\n    if (this.axis.length !== generic_utils.unique(this.axis).length) {\n      throw new Error(`Found duplicate axes in: ${this.axis}`);\n    }\n    const paramShape = this.axis.map(axis => inputShape[axis]);\n    const trainable = true;\n    if (this.scale) {\n      this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n    } else {\n      this.gamma = null;\n    }\n    if (this.center) {\n      this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n    } else {\n      this.beta = null;\n    }\n    this.built = true;\n  }\n  call(inputs, kwargs) {\n    const input = getExactlyOneTensor(inputs);\n    const inputShape = input.shape;\n    const nDims = inputShape.length;\n    return tidy(() => {\n      const keepDims = true;\n      let {\n        mean,\n        variance\n      } = moments(input, this.axis, keepDims);\n      const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n      for (const dim of this.axis) {\n        broadcastShape[dim] = inputShape[dim];\n      }\n      const broadcast = v => {\n        if (v != null && v.shape.length !== nDims) {\n          return tfc.reshape(v, broadcastShape);\n        } else {\n          return v;\n        }\n      };\n      let scale = this.scale ? broadcast(this.gamma.read()) : null;\n      let offset = this.center ? broadcast(this.beta.read()) : null;\n      // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n      // is a workaround for the limitation of core's batchNormalization?d don't\n      // support broadcasting in their gradients. In addition, the tiling is\n      // necessary to ensure correctness on the browser CPU backend regardless\n      // of forward or backward computation. Remove this workaround once the\n      // limitation is addressed. See .\n      const momentsTiling = [];\n      const scaleOffsetTiling = [];\n      for (let i = 0; i < nDims; ++i) {\n        if (this.axis.indexOf(i) !== -1) {\n          momentsTiling.push(inputShape[i]);\n          scaleOffsetTiling.push(1);\n        } else {\n          momentsTiling.push(1);\n          scaleOffsetTiling.push(inputShape[i]);\n        }\n      }\n      mean = tfc.tile(mean, momentsTiling);\n      variance = tfc.tile(variance, momentsTiling);\n      if (scale != null) {\n        scale = tfc.tile(scale, scaleOffsetTiling);\n      }\n      if (offset != null) {\n        offset = tfc.tile(offset, scaleOffsetTiling);\n      }\n      return batchNormalization(input, mean, variance, offset, scale, this.epsilon);\n    });\n  }\n  getConfig() {\n    const config = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\n/** @nocollapse */\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);","map":{"version":3,"names":["tfc","moments","reshape","serialization","tidy","util","getConstraint","serializeConstraint","InputSpec","Layer","NotImplementedError","ValueError","getInitializer","serializeInitializer","getRegularizer","serializeRegularizer","generic_utils","math_utils","getExactlyOneShape","getExactlyOneTensor","batchNormalization","x","mean","variance","beta","gamma","epsilon","arguments","length","undefined","out","rank","batchNorm2d","batchNorm3d","batchNorm4d","regularNormalizeBatchInTraining","reductionAxes","meanAndVariance","normed","broadcastNormalizeBatchInTraining","targetShape","axis","range","indexOf","push","shape","broadcastMean","broadcastVariance","broadcastGamma","broadcastBeta","normalizeBatchInTraining","arraysEqual","slice","sort","BatchNormalization","constructor","args","supportsMasking","momentum","center","scale","betaInitializer","gammaInitializer","movingMeanInitializer","movingVarianceInitializer","betaConstraint","gammaConstraint","betaRegularizer","gammaRegularizer","build","inputShape","dim","JSON","stringify","inputSpec","ndim","axes","addWeight","movingMean","movingVariance","built","call","inputs","kwargs","training","input","splice","broadcastShape","pyListRepeat","sortedReductionAxes","needsBroadcasting","normalizeInference","broadcastMovingMean","read","broadcastMovingVariance","normedTraining","doMovingAverage","variable","value","decay","origValue","updateDelta","mul","sub","write","updateMovingMeanAndVariance","getConfig","config","baseConfig","Object","assign","className","registerClass","LayerNormalization","Number","isInteger","Error","Array","isArray","nDims","i","unique","paramShape","map","trainable","keepDims","broadcast","v","offset","momentsTiling","scaleOffsetTiling","tile"],"sources":["/Users/minjeongyeom/Projects/project-wildet/tfjs-layers/src/layers/normalization.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Normalization layers.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {moments, reshape, serialization, Tensor, Tensor1D, Tensor2D, Tensor3D, Tensor4D, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {Constraint, ConstraintIdentifier, getConstraint, serializeConstraint} from '../constraints';\nimport {InputSpec, Layer, LayerArgs} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, serializeInitializer} from '../initializers';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, RegularizerIdentifier, serializeRegularizer} from '../regularizers';\nimport {Kwargs} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(\n    x: Tensor, mean: Tensor, variance: Tensor, beta?: Tensor, gamma?: Tensor,\n    epsilon = 1e-3): Tensor {\n  let out: Tensor;\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(\n        x as Tensor2D, mean as Tensor2D | Tensor1D,\n        variance as Tensor2D | Tensor1D, beta as Tensor2D | Tensor1D,\n        gamma as Tensor2D | Tensor1D, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(\n        x as Tensor3D, mean as Tensor3D | Tensor1D,\n        variance as Tensor3D | Tensor1D, beta as Tensor3D | Tensor1D,\n        gamma as Tensor3D | Tensor1D, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(\n        x as Tensor4D, mean as Tensor4D | Tensor1D,\n        variance as Tensor4D | Tensor1D, beta as Tensor4D | Tensor1D,\n        gamma as Tensor4D | Tensor1D, epsilon);\n  } else {\n    throw new NotImplementedError(\n        `batchNormalization is not implemented for array of rank ${x.rank} ` +\n        `yet`);\n  }\n  return out;\n}\n\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  return tidy(() => {\n           const meanAndVariance = tfc.moments(x, reductionAxes);\n           const mean = meanAndVariance.mean;\n           const variance = meanAndVariance.variance;\n           const normed =\n               batchNormalization(x, mean, variance, beta, gamma, epsilon);\n           return [normed, mean, variance];\n         }) as [Tensor, Tensor, Tensor];\n}\n\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  return tidy(() => {\n           const meanAndVariance = tfc.moments(x, reductionAxes);\n           const mean = meanAndVariance.mean;\n           const variance = meanAndVariance.variance;\n           const targetShape: number[] = [];\n           for (const axis of math_utils.range(0, x.rank)) {\n             if (reductionAxes.indexOf(axis) !== -1) {\n               targetShape.push(1);\n             } else {\n               targetShape.push(x.shape[axis]);\n             }\n           }\n           const broadcastMean = reshape(mean, targetShape);\n           const broadcastVariance = reshape(variance, targetShape);\n           const broadcastGamma =\n               gamma == null ? null : reshape(gamma, targetShape);\n           const broadcastBeta =\n               beta == null ? null : reshape(beta, targetShape);\n           const normed = batchNormalization(\n               x, broadcastMean, broadcastVariance, broadcastBeta,\n               broadcastGamma, epsilon);\n           return [normed, mean, variance];\n         }) as [Tensor, Tensor, Tensor];\n}\n\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  if (util.arraysEqual(\n          reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(\n        x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(\n        x, gamma, beta, reductionAxes, epsilon);\n  }\n}\n\nexport declare interface BatchNormalizationLayerArgs extends LayerArgs {\n  /**\n   * The integer axis that should be normalized (typically the features axis).\n   * Defaults to -1.\n   *\n   * For instance, after a `Conv2D` layer with `data_format=\"channels_first\"`,\n   * set `axis=1` in `batchNormalization`.\n   */\n  axis?: number;\n\n  /**\n   * Momentum of the moving average. Defaults to 0.99.\n   */\n  momentum?: number;\n\n  /**\n   * Small float added to the variance to avoid dividing by zero. Defaults to\n   * 1e-3.\n   */\n  epsilon?: number;\n\n  /**\n   * If `true`, add offset of `beta` to normalized tensor.\n   * If `false`, `beta` is ignored.\n   * Defaults to `true`.\n   */\n  center?: boolean;\n\n  /**\n   * If `true`, multiply by `gamma`.\n   * If `false`, `gamma` is not used.\n   * When the next layer is linear (also e.g. `nn.relu`),\n   * this can be disabled since the scaling will be done by the next layer.\n   * Defaults to `true`.\n   */\n  scale?: boolean;\n\n  /**\n   * Initializer for the beta weight.\n   *  Defaults to 'zeros'.\n   */\n  betaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the gamma weight.\n   *  Defaults to `ones`.\n   */\n  gammaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the moving mean.\n   * Defaults to `zeros`\n   */\n  movingMeanInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the moving variance.\n   *  Defaults to 'Ones'.\n   */\n  movingVarianceInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Constraint for the beta weight.\n   */\n  betaConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint for gamma weight.\n   */\n  gammaConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Regularizer for the beta weight.\n   */\n  betaRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer for the gamma weight.\n   */\n  gammaRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport class BatchNormalization extends Layer {\n  /** @nocollapse */\n  static className = 'BatchNormalization';\n  private readonly axis: number;\n  private readonly momentum: number;\n  private readonly epsilon: number;\n  private readonly center: boolean;\n  private readonly scale: boolean;\n  private readonly betaInitializer: Initializer;\n  private readonly gammaInitializer: Initializer;\n  private readonly movingMeanInitializer: Initializer;\n  private readonly movingVarianceInitializer: Initializer;\n  private readonly betaConstraint: Constraint;\n  private readonly gammaConstraint: Constraint;\n  private readonly betaRegularizer: Regularizer;\n  private readonly gammaRegularizer: Regularizer;\n  private gamma: LayerVariable;\n  private beta: LayerVariable;\n  private movingMean: LayerVariable;\n  private movingVariance: LayerVariable;\n\n  constructor(args?: BatchNormalizationLayerArgs) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n\n    this.supportsMasking = true;\n    this.axis = args.axis == null ? -1 : args.axis;\n    this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.movingMeanInitializer =\n        getInitializer(args.movingMeanInitializer || 'zeros');\n    this.movingVarianceInitializer =\n        getInitializer(args.movingVarianceInitializer || 'ones');\n    this.betaConstraint = getConstraint(args.betaConstraint);\n    this.gammaConstraint = getConstraint(args.gammaConstraint);\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n  }\n\n  public override build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n    const dim = inputShape[axis];\n    if (dim == null) {\n      throw new ValueError(\n          `Axis ${axis} of input tensor should have a defined dimension but ` +\n          `the layer received an input with shape ` +\n          `${JSON.stringify(inputShape)}.`);\n    }\n    this.inputSpec =\n        [new InputSpec({ndim: inputShape.length, axes: {[axis]: dim}})];\n    const shape = [dim];\n    if (this.scale) {\n      this.gamma = this.addWeight(\n          'gamma', shape, null, this.gammaInitializer, this.gammaRegularizer,\n          true, this.gammaConstraint);\n    }\n    if (this.center) {\n      this.beta = this.addWeight(\n          'beta', shape, null, this.betaInitializer, this.betaRegularizer, true,\n          this.betaConstraint);\n    }\n    this.movingMean = this.addWeight(\n        'moving_mean', shape, null, this.movingMeanInitializer, null, false);\n    this.movingVariance = this.addWeight(\n        'moving_variance', shape, null, this.movingVarianceInitializer, null,\n        false);\n    this.built = true;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const ndim = inputShape.length;\n      const reductionAxes = math_utils.range(0, ndim);\n      const axis = this.axis >= 0 ? this.axis : (this.axis + ndim);\n      reductionAxes.splice(axis, 1);\n      const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n      broadcastShape[axis] = inputShape[axis];\n\n      const sortedReductionAxes = reductionAxes.slice();\n      sortedReductionAxes.sort();\n      const needsBroadcasting = !util.arraysEqual(\n          sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n\n      const normalizeInference: () => Tensor = () => {\n        if (needsBroadcasting) {\n          const broadcastMovingMean =\n              reshape(this.movingMean.read(), broadcastShape);\n          const broadcastMovingVariance =\n              reshape(this.movingVariance.read(), broadcastShape);\n          const broadcastBeta =\n              this.center ? reshape(this.beta.read(), broadcastShape) : null;\n          const broadcastGamma =\n              this.scale ? reshape(this.gamma.read(), broadcastShape) : null;\n          return batchNormalization(\n              input, broadcastMovingMean, broadcastMovingVariance,\n              broadcastBeta, broadcastGamma, this.epsilon);\n        } else {\n          return batchNormalization(\n              input, this.movingMean.read(), this.movingVariance.read(),\n              this.beta == null ? null : this.beta.read(),\n              this.gamma == null ? null : this.gamma.read(), this.epsilon);\n        }\n      };\n\n      if (!training) {\n        return normalizeInference();\n      }\n\n      const [normedTraining, mean, variance] = normalizeBatchInTraining(\n          input, this.gamma.read(), this.beta.read(), reductionAxes,\n          this.epsilon);\n\n      const doMovingAverage =\n          (variable: LayerVariable, value: Tensor, momentum: number): void => {\n            tfc.tidy(() => {\n              const decay = 1 - momentum;\n              const origValue = variable.read();\n              const updateDelta = tfc.mul(tfc.sub(origValue, value), decay);\n              variable.write(tfc.sub(origValue, updateDelta));\n            });\n          };\n\n      // Perform updates to moving mean and moving variance for training.\n      // Porting Note: In PyKeras, these updates to `movingMean` and\n      //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n      //   `update`s using the `add_update()` method. Here we do it imperatively\n      //   and encapsulate the updates in a function that is invoked\n      //   immediately.\n      const updateMovingMeanAndVariance = () => {\n        doMovingAverage(this.movingMean, mean, this.momentum);\n        doMovingAverage(this.movingVariance, variance, this.momentum);\n      };\n      updateMovingMeanAndVariance();\n\n      return normedTraining;\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer:\n          serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(BatchNormalization);\n\nexport interface LayerNormalizationLayerArgs extends LayerArgs {\n  /**\n   * The axis or axes that should be normalized (typically, the feature axis).\n   * Defaults to -1 (the last axis).\n   */\n  axis?: number|number[];\n\n  /**\n   * A small positive float added to variance to avoid divison by zero.\n   * Defaults to 1e-3.\n   */\n  epsilon?: number;\n\n  /**\n   * If `true`, add offset of `beta` to normalized tensor.\n   * If `false`, `beta` is ignored.\n   * Default: `true`.\n   */\n  center?: boolean;\n\n  /**\n   * If `true`, multiply output by `gamma`.\n   * If `false`, `gamma` is not used.\n   * When the next layer is linear, this can be disabled since scaling will\n   * be done by the next layer.\n   * Default: `true`.\n   */\n  scale?: boolean;\n\n  /**\n   * Initializer for the beta weight.\n   * Default: `'zeros'`.\n   */\n  betaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the gamma weight.\n   * Default: `'ones'`.\n   */\n  gammaInitializer?: InitializerIdentifier|Initializer;\n\n  /** Regularizer for the beta weight. */\n  betaRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /** Regularizer for the gamma weight. */\n  gammaRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport class LayerNormalization extends Layer {\n  /** @nocollapse */\n  static className = 'LayerNormalization';\n\n  private axis: number|number[];\n  readonly epsilon: number;\n  readonly center: boolean;\n  readonly scale: boolean;\n  readonly betaInitializer: Initializer;\n  readonly gammaInitializer: Initializer;\n  readonly betaRegularizer: Regularizer;\n  readonly gammaRegularizer: Regularizer;\n\n  private gamma: LayerVariable;\n  private beta: LayerVariable;\n\n  constructor(args?: LayerNormalizationLayerArgs) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n\n    this.axis = args.axis == null ? -1 : args.axis;\n    if (typeof this.axis === 'number') {\n      if (!Number.isInteger(this.axis)) {\n        throw new Error(\n            `Expected axis to be an integer, but received ${this.axis}`);\n      }\n    } else if (Array.isArray(this.axis)) {\n      for (const axis of this.axis) {\n        if (!Number.isInteger(axis)) {\n          throw new Error(\n              `Expected axis to be an array of integers, ` +\n              `but received ${JSON.stringify(this.axis)}`);\n        }\n      }\n    } else {\n      throw new Error(\n          `Expected axis to be an integer or an array of integers, ` +\n          `but received ${JSON.stringify(this.axis)}`);\n    }\n\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n\n    this.supportsMasking = true;\n  }\n\n  public override build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const nDims = inputShape.length;\n\n    // Convert axis to array and resolve negatives.\n    if (typeof this.axis === 'number') {\n      this.axis = [this.axis];\n    }\n    for (let i = 0; i < this.axis.length; ++i) {\n      if (this.axis[i] < 0) {\n        this.axis[i] += nDims;\n      }\n    }\n\n    // Further validate axes.\n    for (const axis of this.axis) {\n      if (axis < 0 || axis >= nDims) {\n        throw new Error(`Invalid axis: ${axis}`);\n      }\n    }\n    if (this.axis.length !== generic_utils.unique(this.axis).length) {\n      throw new Error(`Found duplicate axes in: ${this.axis}`);\n    }\n\n    const paramShape = this.axis.map(axis => inputShape[axis]) as number[];\n\n    const trainable = true;\n    if (this.scale) {\n      this.gamma = this.addWeight(\n          'gamma', paramShape, 'float32', this.gammaInitializer,\n          this.gammaRegularizer, trainable);\n    } else {\n      this.gamma = null;\n    }\n    if (this.center) {\n      this.beta = this.addWeight(\n          'beta', paramShape, 'float32', this.betaInitializer,\n          this.betaRegularizer, trainable);\n    } else {\n      this.beta = null;\n    }\n\n    this.built = true;\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const input = getExactlyOneTensor(inputs);\n    const inputShape = input.shape;\n    const nDims = inputShape.length;\n\n    return tidy(() => {\n      const keepDims = true;\n      let {mean, variance} = moments(input, this.axis, keepDims);\n      const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n      for (const dim of this.axis as number[]) {\n        broadcastShape[dim] = inputShape[dim];\n      }\n\n      const broadcast = (v: Tensor) => {\n        if (v != null && v.shape.length !== nDims) {\n          return tfc.reshape(v, broadcastShape);\n        } else {\n          return v;\n        }\n      };\n\n      let scale = this.scale ? broadcast(this.gamma.read()) : null;\n      let offset = this.center ? broadcast(this.beta.read()) : null;\n\n      // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n      // is a workaround for the limitation of core's batchNormalization?d don't\n      // support broadcasting in their gradients. In addition, the tiling is\n      // necessary to ensure correctness on the browser CPU backend regardless\n      // of forward or backward computation. Remove this workaround once the\n      // limitation is addressed. See .\n      const momentsTiling: number[] = [];\n      const scaleOffsetTiling: number[] = [];\n      for (let i = 0; i < nDims; ++i) {\n        if ((this.axis as number[]).indexOf(i) !== -1) {\n          momentsTiling.push(inputShape[i]);\n          scaleOffsetTiling.push(1);\n        } else {\n          momentsTiling.push(1);\n          scaleOffsetTiling.push(inputShape[i]);\n        }\n      }\n      mean = tfc.tile(mean, momentsTiling);\n      variance = tfc.tile(variance, momentsTiling);\n      if (scale != null) {\n        scale = tfc.tile(scale, scaleOffsetTiling);\n      }\n      if (offset != null) {\n        offset = tfc.tile(offset, scaleOffsetTiling);\n      }\n\n      return batchNormalization(\n          input, mean, variance, offset, scale, this.epsilon);\n    });\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(LayerNormalization);\n"],"mappings":"AAAA;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAAQC,OAAO,EAAEC,OAAO,EAAEC,aAAa,EAAkDC,IAAI,EAAEC,IAAI,QAAO,uBAAuB;AAEjI,SAA0CC,aAAa,EAAEC,mBAAmB,QAAO,gBAAgB;AACnG,SAAQC,SAAS,EAAEC,KAAK,QAAkB,oBAAoB;AAC9D,SAAQC,mBAAmB,EAAEC,UAAU,QAAO,WAAW;AACzD,SAAQC,cAAc,EAAsCC,oBAAoB,QAAO,iBAAiB;AAExG,SAAQC,cAAc,EAAsCC,oBAAoB,QAAO,iBAAiB;AAExG,OAAO,KAAKC,aAAa,MAAM,wBAAwB;AACvD,OAAO,KAAKC,UAAU,MAAM,qBAAqB;AACjD,SAAQC,kBAAkB,EAAEC,mBAAmB,QAAO,sBAAsB;AAG5E;;;;;;;;;;;;;;AAcA,OAAM,SAAUC,kBAAkBA,CAC9BC,CAAS,EAAEC,IAAY,EAAEC,QAAgB,EAAEC,IAAa,EAAEC,KAAc,EAC1D;EAAA,IAAdC,OAAO,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;EAChB,IAAIG,GAAW;EACf,IAAIT,CAAC,CAACU,IAAI,KAAK,CAAC,EAAE;IAChBD,GAAG,GAAG9B,GAAG,CAACgC,WAAW,CACjBX,CAAa,EAAEC,IAA2B,EAC1CC,QAA+B,EAAEC,IAA2B,EAC5DC,KAA4B,EAAEC,OAAO,CAAC;GAC3C,MAAM,IAAIL,CAAC,CAACU,IAAI,KAAK,CAAC,EAAE;IACvB;IACAD,GAAG,GAAG9B,GAAG,CAACiC,WAAW,CACjBZ,CAAa,EAAEC,IAA2B,EAC1CC,QAA+B,EAAEC,IAA2B,EAC5DC,KAA4B,EAAEC,OAAO,CAAC;GAC3C,MAAM,IAAIL,CAAC,CAACU,IAAI,KAAK,CAAC,EAAE;IACvBD,GAAG,GAAG9B,GAAG,CAACkC,WAAW,CACjBb,CAAa,EAAEC,IAA2B,EAC1CC,QAA+B,EAAEC,IAA2B,EAC5DC,KAA4B,EAAEC,OAAO,CAAC;GAC3C,MAAM;IACL,MAAM,IAAIhB,mBAAmB,CACzB,2DAA2DW,CAAC,CAACU,IAAI,GAAG,GACpE,KAAK,CAAC;;EAEZ,OAAOD,GAAG;AACZ;AAEA;;;;;;;;;;;;;;;;;AAiBA,SAASK,+BAA+BA,CACpCd,CAAS,EAAEI,KAAa,EAAED,IAAY,EAAEY,aAAuB,EACjD;EAAA,IAAdV,OAAO,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;EAChB,OAAOvB,IAAI,CAAC,MAAK;IACR,MAAMiC,eAAe,GAAGrC,GAAG,CAACC,OAAO,CAACoB,CAAC,EAAEe,aAAa,CAAC;IACrD,MAAMd,IAAI,GAAGe,eAAe,CAACf,IAAI;IACjC,MAAMC,QAAQ,GAAGc,eAAe,CAACd,QAAQ;IACzC,MAAMe,MAAM,GACRlB,kBAAkB,CAACC,CAAC,EAAEC,IAAI,EAAEC,QAAQ,EAAEC,IAAI,EAAEC,KAAK,EAAEC,OAAO,CAAC;IAC/D,OAAO,CAACY,MAAM,EAAEhB,IAAI,EAAEC,QAAQ,CAAC;EACjC,CAAC,CAA6B;AACvC;AAEA;;;;;;;;;;;;;;;;;AAiBA,SAASgB,iCAAiCA,CACtClB,CAAS,EAAEI,KAAa,EAAED,IAAY,EAAEY,aAAuB,EACjD;EAAA,IAAdV,OAAO,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;EAChB,OAAOvB,IAAI,CAAC,MAAK;IACR,MAAMiC,eAAe,GAAGrC,GAAG,CAACC,OAAO,CAACoB,CAAC,EAAEe,aAAa,CAAC;IACrD,MAAMd,IAAI,GAAGe,eAAe,CAACf,IAAI;IACjC,MAAMC,QAAQ,GAAGc,eAAe,CAACd,QAAQ;IACzC,MAAMiB,WAAW,GAAa,EAAE;IAChC,KAAK,MAAMC,IAAI,IAAIxB,UAAU,CAACyB,KAAK,CAAC,CAAC,EAAErB,CAAC,CAACU,IAAI,CAAC,EAAE;MAC9C,IAAIK,aAAa,CAACO,OAAO,CAACF,IAAI,CAAC,KAAK,CAAC,CAAC,EAAE;QACtCD,WAAW,CAACI,IAAI,CAAC,CAAC,CAAC;OACpB,MAAM;QACLJ,WAAW,CAACI,IAAI,CAACvB,CAAC,CAACwB,KAAK,CAACJ,IAAI,CAAC,CAAC;;;IAGnC,MAAMK,aAAa,GAAG5C,OAAO,CAACoB,IAAI,EAAEkB,WAAW,CAAC;IAChD,MAAMO,iBAAiB,GAAG7C,OAAO,CAACqB,QAAQ,EAAEiB,WAAW,CAAC;IACxD,MAAMQ,cAAc,GAChBvB,KAAK,IAAI,IAAI,GAAG,IAAI,GAAGvB,OAAO,CAACuB,KAAK,EAAEe,WAAW,CAAC;IACtD,MAAMS,aAAa,GACfzB,IAAI,IAAI,IAAI,GAAG,IAAI,GAAGtB,OAAO,CAACsB,IAAI,EAAEgB,WAAW,CAAC;IACpD,MAAMF,MAAM,GAAGlB,kBAAkB,CAC7BC,CAAC,EAAEyB,aAAa,EAAEC,iBAAiB,EAAEE,aAAa,EAClDD,cAAc,EAAEtB,OAAO,CAAC;IAC5B,OAAO,CAACY,MAAM,EAAEhB,IAAI,EAAEC,QAAQ,CAAC;EACjC,CAAC,CAA6B;AACvC;AAEA;;;;;;;;;;;AAWA,OAAM,SAAU2B,wBAAwBA,CACpC7B,CAAS,EAAEI,KAAa,EAAED,IAAY,EAAEY,aAAuB,EACjD;EAAA,IAAdV,OAAO,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,IAAI;EAChB,IAAItB,IAAI,CAAC8C,WAAW,CACZf,aAAa,CAACgB,KAAK,EAAE,CAACC,IAAI,EAAE,EAAEpC,UAAU,CAACyB,KAAK,CAAC,CAAC,EAAErB,CAAC,CAACU,IAAI,GAAG,CAAC,CAAC,CAAC,EAAE;IACtE,OAAOI,+BAA+B,CAClCd,CAAC,EAAEI,KAAK,EAAED,IAAI,EAAEY,aAAa,EAAEV,OAAO,CAAC;GAC5C,MAAM;IACL,OAAOa,iCAAiC,CACpClB,CAAC,EAAEI,KAAK,EAAED,IAAI,EAAEY,aAAa,EAAEV,OAAO,CAAC;;AAE/C;AAoFA,OAAM,MAAO4B,kBAAmB,SAAQ7C,KAAK;EAqB3C8C,YAAYC,IAAkC;IAC5C,IAAIA,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG,EAAE;;IAEX,KAAK,CAACA,IAAI,CAAC;IAEX,IAAI,CAACC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAAChB,IAAI,GAAGe,IAAI,CAACf,IAAI,IAAI,IAAI,GAAG,CAAC,CAAC,GAAGe,IAAI,CAACf,IAAI;IAC9C,IAAI,CAACiB,QAAQ,GAAGF,IAAI,CAACE,QAAQ,IAAI,IAAI,GAAG,IAAI,GAAGF,IAAI,CAACE,QAAQ;IAC5D,IAAI,CAAChC,OAAO,GAAG8B,IAAI,CAAC9B,OAAO,IAAI,IAAI,GAAG,IAAI,GAAG8B,IAAI,CAAC9B,OAAO;IACzD,IAAI,CAACiC,MAAM,GAAGH,IAAI,CAACG,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGH,IAAI,CAACG,MAAM;IACtD,IAAI,CAACC,KAAK,GAAGJ,IAAI,CAACI,KAAK,IAAI,IAAI,GAAG,IAAI,GAAGJ,IAAI,CAACI,KAAK;IACnD,IAAI,CAACC,eAAe,GAAGjD,cAAc,CAAC4C,IAAI,CAACK,eAAe,IAAI,OAAO,CAAC;IACtE,IAAI,CAACC,gBAAgB,GAAGlD,cAAc,CAAC4C,IAAI,CAACM,gBAAgB,IAAI,MAAM,CAAC;IACvE,IAAI,CAACC,qBAAqB,GACtBnD,cAAc,CAAC4C,IAAI,CAACO,qBAAqB,IAAI,OAAO,CAAC;IACzD,IAAI,CAACC,yBAAyB,GAC1BpD,cAAc,CAAC4C,IAAI,CAACQ,yBAAyB,IAAI,MAAM,CAAC;IAC5D,IAAI,CAACC,cAAc,GAAG3D,aAAa,CAACkD,IAAI,CAACS,cAAc,CAAC;IACxD,IAAI,CAACC,eAAe,GAAG5D,aAAa,CAACkD,IAAI,CAACU,eAAe,CAAC;IAC1D,IAAI,CAACC,eAAe,GAAGrD,cAAc,CAAC0C,IAAI,CAACW,eAAe,CAAC;IAC3D,IAAI,CAACC,gBAAgB,GAAGtD,cAAc,CAAC0C,IAAI,CAACY,gBAAgB,CAAC;EAC/D;EAEgBC,KAAKA,CAACC,UAAyB;IAC7CA,UAAU,GAAGpD,kBAAkB,CAACoD,UAAU,CAAC;IAC3C,MAAM7B,IAAI,GAAG,IAAI,CAACA,IAAI,IAAI,CAAC,GAAG,IAAI,CAACA,IAAI,GAAI,IAAI,CAACA,IAAI,GAAG6B,UAAU,CAAC1C,MAAO;IACzE,MAAM2C,GAAG,GAAGD,UAAU,CAAC7B,IAAI,CAAC;IAC5B,IAAI8B,GAAG,IAAI,IAAI,EAAE;MACf,MAAM,IAAI5D,UAAU,CAChB,QAAQ8B,IAAI,uDAAuD,GACnE,yCAAyC,GACzC,GAAG+B,IAAI,CAACC,SAAS,CAACH,UAAU,CAAC,GAAG,CAAC;;IAEvC,IAAI,CAACI,SAAS,GACV,CAAC,IAAIlE,SAAS,CAAC;MAACmE,IAAI,EAAEL,UAAU,CAAC1C,MAAM;MAAEgD,IAAI,EAAE;QAAC,CAACnC,IAAI,GAAG8B;MAAG;IAAC,CAAC,CAAC,CAAC;IACnE,MAAM1B,KAAK,GAAG,CAAC0B,GAAG,CAAC;IACnB,IAAI,IAAI,CAACX,KAAK,EAAE;MACd,IAAI,CAACnC,KAAK,GAAG,IAAI,CAACoD,SAAS,CACvB,OAAO,EAAEhC,KAAK,EAAE,IAAI,EAAE,IAAI,CAACiB,gBAAgB,EAAE,IAAI,CAACM,gBAAgB,EAClE,IAAI,EAAE,IAAI,CAACF,eAAe,CAAC;;IAEjC,IAAI,IAAI,CAACP,MAAM,EAAE;MACf,IAAI,CAACnC,IAAI,GAAG,IAAI,CAACqD,SAAS,CACtB,MAAM,EAAEhC,KAAK,EAAE,IAAI,EAAE,IAAI,CAACgB,eAAe,EAAE,IAAI,CAACM,eAAe,EAAE,IAAI,EACrE,IAAI,CAACF,cAAc,CAAC;;IAE1B,IAAI,CAACa,UAAU,GAAG,IAAI,CAACD,SAAS,CAC5B,aAAa,EAAEhC,KAAK,EAAE,IAAI,EAAE,IAAI,CAACkB,qBAAqB,EAAE,IAAI,EAAE,KAAK,CAAC;IACxE,IAAI,CAACgB,cAAc,GAAG,IAAI,CAACF,SAAS,CAChC,iBAAiB,EAAEhC,KAAK,EAAE,IAAI,EAAE,IAAI,CAACmB,yBAAyB,EAAE,IAAI,EACpE,KAAK,CAAC;IACV,IAAI,CAACgB,KAAK,GAAG,IAAI;EACnB;EAESC,IAAIA,CAACC,MAAuB,EAAEC,MAAc;IACnD,OAAO/E,IAAI,CAAC,MAAK;MACf,MAAMgF,QAAQ,GAAGD,MAAM,CAAC,UAAU,CAAC,IAAI,IAAI,GAAG,KAAK,GAAGA,MAAM,CAAC,UAAU,CAAC;MACxE,MAAME,KAAK,GAAGlE,mBAAmB,CAAC+D,MAAM,CAAC;MACzC,MAAMZ,UAAU,GAAGe,KAAK,CAACxC,KAAK;MAC9B,MAAM8B,IAAI,GAAGL,UAAU,CAAC1C,MAAM;MAC9B,MAAMQ,aAAa,GAAGnB,UAAU,CAACyB,KAAK,CAAC,CAAC,EAAEiC,IAAI,CAAC;MAC/C,MAAMlC,IAAI,GAAG,IAAI,CAACA,IAAI,IAAI,CAAC,GAAG,IAAI,CAACA,IAAI,GAAI,IAAI,CAACA,IAAI,GAAGkC,IAAK;MAC5DvC,aAAa,CAACkD,MAAM,CAAC7C,IAAI,EAAE,CAAC,CAAC;MAC7B,MAAM8C,cAAc,GAAGvE,aAAa,CAACwE,YAAY,CAAC,CAAC,EAAEb,IAAI,CAAC;MAC1DY,cAAc,CAAC9C,IAAI,CAAC,GAAG6B,UAAU,CAAC7B,IAAI,CAAC;MAEvC,MAAMgD,mBAAmB,GAAGrD,aAAa,CAACgB,KAAK,EAAE;MACjDqC,mBAAmB,CAACpC,IAAI,EAAE;MAC1B,MAAMqC,iBAAiB,GAAG,CAACrF,IAAI,CAAC8C,WAAW,CACvCsC,mBAAmB,EAAExE,UAAU,CAACyB,KAAK,CAAC,CAAC,EAAEiC,IAAI,CAAC,CAACvB,KAAK,CAAC,CAAC,EAAEuB,IAAI,GAAG,CAAC,CAAC,CAAC;MAEtE,MAAMgB,kBAAkB,GAAiBA,CAAA,KAAK;QAC5C,IAAID,iBAAiB,EAAE;UACrB,MAAME,mBAAmB,GACrB1F,OAAO,CAAC,IAAI,CAAC4E,UAAU,CAACe,IAAI,EAAE,EAAEN,cAAc,CAAC;UACnD,MAAMO,uBAAuB,GACzB5F,OAAO,CAAC,IAAI,CAAC6E,cAAc,CAACc,IAAI,EAAE,EAAEN,cAAc,CAAC;UACvD,MAAMtC,aAAa,GACf,IAAI,CAACU,MAAM,GAAGzD,OAAO,CAAC,IAAI,CAACsB,IAAI,CAACqE,IAAI,EAAE,EAAEN,cAAc,CAAC,GAAG,IAAI;UAClE,MAAMvC,cAAc,GAChB,IAAI,CAACY,KAAK,GAAG1D,OAAO,CAAC,IAAI,CAACuB,KAAK,CAACoE,IAAI,EAAE,EAAEN,cAAc,CAAC,GAAG,IAAI;UAClE,OAAOnE,kBAAkB,CACrBiE,KAAK,EAAEO,mBAAmB,EAAEE,uBAAuB,EACnD7C,aAAa,EAAED,cAAc,EAAE,IAAI,CAACtB,OAAO,CAAC;SACjD,MAAM;UACL,OAAON,kBAAkB,CACrBiE,KAAK,EAAE,IAAI,CAACP,UAAU,CAACe,IAAI,EAAE,EAAE,IAAI,CAACd,cAAc,CAACc,IAAI,EAAE,EACzD,IAAI,CAACrE,IAAI,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAACA,IAAI,CAACqE,IAAI,EAAE,EAC3C,IAAI,CAACpE,KAAK,IAAI,IAAI,GAAG,IAAI,GAAG,IAAI,CAACA,KAAK,CAACoE,IAAI,EAAE,EAAE,IAAI,CAACnE,OAAO,CAAC;;MAEpE,CAAC;MAED,IAAI,CAAC0D,QAAQ,EAAE;QACb,OAAOO,kBAAkB,EAAE;;MAG7B,MAAM,CAACI,cAAc,EAAEzE,IAAI,EAAEC,QAAQ,CAAC,GAAG2B,wBAAwB,CAC7DmC,KAAK,EAAE,IAAI,CAAC5D,KAAK,CAACoE,IAAI,EAAE,EAAE,IAAI,CAACrE,IAAI,CAACqE,IAAI,EAAE,EAAEzD,aAAa,EACzD,IAAI,CAACV,OAAO,CAAC;MAEjB,MAAMsE,eAAe,GACjBA,CAACC,QAAuB,EAAEC,KAAa,EAAExC,QAAgB,KAAU;QACjE1D,GAAG,CAACI,IAAI,CAAC,MAAK;UACZ,MAAM+F,KAAK,GAAG,CAAC,GAAGzC,QAAQ;UAC1B,MAAM0C,SAAS,GAAGH,QAAQ,CAACJ,IAAI,EAAE;UACjC,MAAMQ,WAAW,GAAGrG,GAAG,CAACsG,GAAG,CAACtG,GAAG,CAACuG,GAAG,CAACH,SAAS,EAAEF,KAAK,CAAC,EAAEC,KAAK,CAAC;UAC7DF,QAAQ,CAACO,KAAK,CAACxG,GAAG,CAACuG,GAAG,CAACH,SAAS,EAAEC,WAAW,CAAC,CAAC;QACjD,CAAC,CAAC;MACJ,CAAC;MAEL;MACA;MACA;MACA;MACA;MACA;MACA,MAAMI,2BAA2B,GAAGA,CAAA,KAAK;QACvCT,eAAe,CAAC,IAAI,CAAClB,UAAU,EAAExD,IAAI,EAAE,IAAI,CAACoC,QAAQ,CAAC;QACrDsC,eAAe,CAAC,IAAI,CAACjB,cAAc,EAAExD,QAAQ,EAAE,IAAI,CAACmC,QAAQ,CAAC;MAC/D,CAAC;MACD+C,2BAA2B,EAAE;MAE7B,OAAOV,cAAc;IACvB,CAAC,CAAC;EACJ;EAESW,SAASA,CAAA;IAChB,MAAMC,MAAM,GAA6B;MACvClE,IAAI,EAAE,IAAI,CAACA,IAAI;MACfiB,QAAQ,EAAE,IAAI,CAACA,QAAQ;MACvBhC,OAAO,EAAE,IAAI,CAACA,OAAO;MACrBiC,MAAM,EAAE,IAAI,CAACA,MAAM;MACnBC,KAAK,EAAE,IAAI,CAACA,KAAK;MACjBC,eAAe,EAAEhD,oBAAoB,CAAC,IAAI,CAACgD,eAAe,CAAC;MAC3DC,gBAAgB,EAAEjD,oBAAoB,CAAC,IAAI,CAACiD,gBAAgB,CAAC;MAC7DC,qBAAqB,EAAElD,oBAAoB,CAAC,IAAI,CAACkD,qBAAqB,CAAC;MACvEC,yBAAyB,EACrBnD,oBAAoB,CAAC,IAAI,CAACmD,yBAAyB,CAAC;MACxDG,eAAe,EAAEpD,oBAAoB,CAAC,IAAI,CAACoD,eAAe,CAAC;MAC3DC,gBAAgB,EAAErD,oBAAoB,CAAC,IAAI,CAACqD,gBAAgB,CAAC;MAC7DH,cAAc,EAAE1D,mBAAmB,CAAC,IAAI,CAAC0D,cAAc,CAAC;MACxDC,eAAe,EAAE3D,mBAAmB,CAAC,IAAI,CAAC2D,eAAe;KAC1D;IACD,MAAM0C,UAAU,GAAG,KAAK,CAACF,SAAS,EAAE;IACpCG,MAAM,CAACC,MAAM,CAACH,MAAM,EAAEC,UAAU,CAAC;IACjC,OAAOD,MAAM;EACf;;AAvKA;AACOrD,kBAAA,CAAAyD,SAAS,GAAG,oBAAoB;AAwKzC5G,aAAa,CAAC6G,aAAa,CAAC1D,kBAAkB,CAAC;AAkD/C,OAAM,MAAO2D,kBAAmB,SAAQxG,KAAK;EAgB3C8C,YAAYC,IAAkC;IAC5C,IAAIA,IAAI,IAAI,IAAI,EAAE;MAChBA,IAAI,GAAG,EAAE;;IAEX,KAAK,CAACA,IAAI,CAAC;IAEX,IAAI,CAACf,IAAI,GAAGe,IAAI,CAACf,IAAI,IAAI,IAAI,GAAG,CAAC,CAAC,GAAGe,IAAI,CAACf,IAAI;IAC9C,IAAI,OAAO,IAAI,CAACA,IAAI,KAAK,QAAQ,EAAE;MACjC,IAAI,CAACyE,MAAM,CAACC,SAAS,CAAC,IAAI,CAAC1E,IAAI,CAAC,EAAE;QAChC,MAAM,IAAI2E,KAAK,CACX,gDAAgD,IAAI,CAAC3E,IAAI,EAAE,CAAC;;KAEnE,MAAM,IAAI4E,KAAK,CAACC,OAAO,CAAC,IAAI,CAAC7E,IAAI,CAAC,EAAE;MACnC,KAAK,MAAMA,IAAI,IAAI,IAAI,CAACA,IAAI,EAAE;QAC5B,IAAI,CAACyE,MAAM,CAACC,SAAS,CAAC1E,IAAI,CAAC,EAAE;UAC3B,MAAM,IAAI2E,KAAK,CACX,4CAA4C,GAC5C,gBAAgB5C,IAAI,CAACC,SAAS,CAAC,IAAI,CAAChC,IAAI,CAAC,EAAE,CAAC;;;KAGrD,MAAM;MACL,MAAM,IAAI2E,KAAK,CACX,0DAA0D,GAC1D,gBAAgB5C,IAAI,CAACC,SAAS,CAAC,IAAI,CAAChC,IAAI,CAAC,EAAE,CAAC;;IAGlD,IAAI,CAACf,OAAO,GAAG8B,IAAI,CAAC9B,OAAO,IAAI,IAAI,GAAG,IAAI,GAAG8B,IAAI,CAAC9B,OAAO;IACzD,IAAI,CAACiC,MAAM,GAAGH,IAAI,CAACG,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGH,IAAI,CAACG,MAAM;IACtD,IAAI,CAACC,KAAK,GAAGJ,IAAI,CAACI,KAAK,IAAI,IAAI,GAAG,IAAI,GAAGJ,IAAI,CAACI,KAAK;IACnD,IAAI,CAACC,eAAe,GAAGjD,cAAc,CAAC4C,IAAI,CAACK,eAAe,IAAI,OAAO,CAAC;IACtE,IAAI,CAACC,gBAAgB,GAAGlD,cAAc,CAAC4C,IAAI,CAACM,gBAAgB,IAAI,MAAM,CAAC;IACvE,IAAI,CAACK,eAAe,GAAGrD,cAAc,CAAC0C,IAAI,CAACW,eAAe,CAAC;IAC3D,IAAI,CAACC,gBAAgB,GAAGtD,cAAc,CAAC0C,IAAI,CAACY,gBAAgB,CAAC;IAE7D,IAAI,CAACX,eAAe,GAAG,IAAI;EAC7B;EAEgBY,KAAKA,CAACC,UAAyB;IAC7CA,UAAU,GAAGpD,kBAAkB,CAACoD,UAAU,CAAC;IAC3C,MAAMiD,KAAK,GAAGjD,UAAU,CAAC1C,MAAM;IAE/B;IACA,IAAI,OAAO,IAAI,CAACa,IAAI,KAAK,QAAQ,EAAE;MACjC,IAAI,CAACA,IAAI,GAAG,CAAC,IAAI,CAACA,IAAI,CAAC;;IAEzB,KAAK,IAAI+E,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAG,IAAI,CAAC/E,IAAI,CAACb,MAAM,EAAE,EAAE4F,CAAC,EAAE;MACzC,IAAI,IAAI,CAAC/E,IAAI,CAAC+E,CAAC,CAAC,GAAG,CAAC,EAAE;QACpB,IAAI,CAAC/E,IAAI,CAAC+E,CAAC,CAAC,IAAID,KAAK;;;IAIzB;IACA,KAAK,MAAM9E,IAAI,IAAI,IAAI,CAACA,IAAI,EAAE;MAC5B,IAAIA,IAAI,GAAG,CAAC,IAAIA,IAAI,IAAI8E,KAAK,EAAE;QAC7B,MAAM,IAAIH,KAAK,CAAC,iBAAiB3E,IAAI,EAAE,CAAC;;;IAG5C,IAAI,IAAI,CAACA,IAAI,CAACb,MAAM,KAAKZ,aAAa,CAACyG,MAAM,CAAC,IAAI,CAAChF,IAAI,CAAC,CAACb,MAAM,EAAE;MAC/D,MAAM,IAAIwF,KAAK,CAAC,4BAA4B,IAAI,CAAC3E,IAAI,EAAE,CAAC;;IAG1D,MAAMiF,UAAU,GAAG,IAAI,CAACjF,IAAI,CAACkF,GAAG,CAAClF,IAAI,IAAI6B,UAAU,CAAC7B,IAAI,CAAC,CAAa;IAEtE,MAAMmF,SAAS,GAAG,IAAI;IACtB,IAAI,IAAI,CAAChE,KAAK,EAAE;MACd,IAAI,CAACnC,KAAK,GAAG,IAAI,CAACoD,SAAS,CACvB,OAAO,EAAE6C,UAAU,EAAE,SAAS,EAAE,IAAI,CAAC5D,gBAAgB,EACrD,IAAI,CAACM,gBAAgB,EAAEwD,SAAS,CAAC;KACtC,MAAM;MACL,IAAI,CAACnG,KAAK,GAAG,IAAI;;IAEnB,IAAI,IAAI,CAACkC,MAAM,EAAE;MACf,IAAI,CAACnC,IAAI,GAAG,IAAI,CAACqD,SAAS,CACtB,MAAM,EAAE6C,UAAU,EAAE,SAAS,EAAE,IAAI,CAAC7D,eAAe,EACnD,IAAI,CAACM,eAAe,EAAEyD,SAAS,CAAC;KACrC,MAAM;MACL,IAAI,CAACpG,IAAI,GAAG,IAAI;;IAGlB,IAAI,CAACwD,KAAK,GAAG,IAAI;EACnB;EAESC,IAAIA,CAACC,MAAuB,EAAEC,MAAc;IACnD,MAAME,KAAK,GAAGlE,mBAAmB,CAAC+D,MAAM,CAAC;IACzC,MAAMZ,UAAU,GAAGe,KAAK,CAACxC,KAAK;IAC9B,MAAM0E,KAAK,GAAGjD,UAAU,CAAC1C,MAAM;IAE/B,OAAOxB,IAAI,CAAC,MAAK;MACf,MAAMyH,QAAQ,GAAG,IAAI;MACrB,IAAI;QAACvG,IAAI;QAAEC;MAAQ,CAAC,GAAGtB,OAAO,CAACoF,KAAK,EAAE,IAAI,CAAC5C,IAAI,EAAEoF,QAAQ,CAAC;MAC1D,MAAMtC,cAAc,GAAGvE,aAAa,CAACwE,YAAY,CAAC,CAAC,EAAE+B,KAAK,CAAC;MAC3D,KAAK,MAAMhD,GAAG,IAAI,IAAI,CAAC9B,IAAgB,EAAE;QACvC8C,cAAc,CAAChB,GAAG,CAAC,GAAGD,UAAU,CAACC,GAAG,CAAC;;MAGvC,MAAMuD,SAAS,GAAIC,CAAS,IAAI;QAC9B,IAAIA,CAAC,IAAI,IAAI,IAAIA,CAAC,CAAClF,KAAK,CAACjB,MAAM,KAAK2F,KAAK,EAAE;UACzC,OAAOvH,GAAG,CAACE,OAAO,CAAC6H,CAAC,EAAExC,cAAc,CAAC;SACtC,MAAM;UACL,OAAOwC,CAAC;;MAEZ,CAAC;MAED,IAAInE,KAAK,GAAG,IAAI,CAACA,KAAK,GAAGkE,SAAS,CAAC,IAAI,CAACrG,KAAK,CAACoE,IAAI,EAAE,CAAC,GAAG,IAAI;MAC5D,IAAImC,MAAM,GAAG,IAAI,CAACrE,MAAM,GAAGmE,SAAS,CAAC,IAAI,CAACtG,IAAI,CAACqE,IAAI,EAAE,CAAC,GAAG,IAAI;MAE7D;MACA;MACA;MACA;MACA;MACA;MACA,MAAMoC,aAAa,GAAa,EAAE;MAClC,MAAMC,iBAAiB,GAAa,EAAE;MACtC,KAAK,IAAIV,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGD,KAAK,EAAE,EAAEC,CAAC,EAAE;QAC9B,IAAK,IAAI,CAAC/E,IAAiB,CAACE,OAAO,CAAC6E,CAAC,CAAC,KAAK,CAAC,CAAC,EAAE;UAC7CS,aAAa,CAACrF,IAAI,CAAC0B,UAAU,CAACkD,CAAC,CAAC,CAAC;UACjCU,iBAAiB,CAACtF,IAAI,CAAC,CAAC,CAAC;SAC1B,MAAM;UACLqF,aAAa,CAACrF,IAAI,CAAC,CAAC,CAAC;UACrBsF,iBAAiB,CAACtF,IAAI,CAAC0B,UAAU,CAACkD,CAAC,CAAC,CAAC;;;MAGzClG,IAAI,GAAGtB,GAAG,CAACmI,IAAI,CAAC7G,IAAI,EAAE2G,aAAa,CAAC;MACpC1G,QAAQ,GAAGvB,GAAG,CAACmI,IAAI,CAAC5G,QAAQ,EAAE0G,aAAa,CAAC;MAC5C,IAAIrE,KAAK,IAAI,IAAI,EAAE;QACjBA,KAAK,GAAG5D,GAAG,CAACmI,IAAI,CAACvE,KAAK,EAAEsE,iBAAiB,CAAC;;MAE5C,IAAIF,MAAM,IAAI,IAAI,EAAE;QAClBA,MAAM,GAAGhI,GAAG,CAACmI,IAAI,CAACH,MAAM,EAAEE,iBAAiB,CAAC;;MAG9C,OAAO9G,kBAAkB,CACrBiE,KAAK,EAAE/D,IAAI,EAAEC,QAAQ,EAAEyG,MAAM,EAAEpE,KAAK,EAAE,IAAI,CAAClC,OAAO,CAAC;IACzD,CAAC,CAAC;EACJ;EAESgF,SAASA,CAAA;IAChB,MAAMC,MAAM,GAA6B;MACvClE,IAAI,EAAE,IAAI,CAACA,IAAI;MACff,OAAO,EAAE,IAAI,CAACA,OAAO;MACrBiC,MAAM,EAAE,IAAI,CAACA,MAAM;MACnBC,KAAK,EAAE,IAAI,CAACA,KAAK;MACjBC,eAAe,EAAEhD,oBAAoB,CAAC,IAAI,CAACgD,eAAe,CAAC;MAC3DC,gBAAgB,EAAEjD,oBAAoB,CAAC,IAAI,CAACiD,gBAAgB,CAAC;MAC7DK,eAAe,EAAEpD,oBAAoB,CAAC,IAAI,CAACoD,eAAe,CAAC;MAC3DC,gBAAgB,EAAErD,oBAAoB,CAAC,IAAI,CAACqD,gBAAgB;KAC7D;IACD,MAAMwC,UAAU,GAAG,KAAK,CAACF,SAAS,EAAE;IACpCG,MAAM,CAACC,MAAM,CAACH,MAAM,EAAEC,UAAU,CAAC;IACjC,OAAOD,MAAM;EACf;;AAtKA;AACOM,kBAAA,CAAAF,SAAS,GAAG,oBAAoB;AAuKzC5G,aAAa,CAAC6G,aAAa,CAACC,kBAAkB,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}